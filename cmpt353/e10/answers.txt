1. How long did your reddit_averages.py take with
  (1) the reddit-0 data set and effectively no work,
    0m11.542s
  (2) no schema specified and not caching (on reddit-2 for this and the rest),
    0m23.225s
  (3) with a schema but not caching,
    0m19.414s
  (4) with both a schema and caching the twice-used DataFrame?
    0m13.809s
[The reddit-0 test is effectively measuring the Spark
startup time, so we can see how long it takes to do the actual
work on reddit-2 in the best/worst cases.]


2. Based on the above, does it look like most of the time taken
to process the reddit-2 data set is in reading the files, or
calculating the averages?

-Looking at the best case scenario (inlcude schema and caching)
reading takes significantly faster.

-If caching and schema not used then it takes about the same amount of time.

3. Where did you use .cache() in your wikipedia_popular.py? [Hint:
the answer had better be “once”… but where?]

I used cache on the dataframe df after I filtered out the pages I don't need. And
before I groupby and join.
I did this because after groupby occurs on df, df is thrown away and needs to be
recalculated when joining.
